# CLAUDE.md - Scan & Speak Multilingual Shopping Assistant

## ğŸ¯ ROLE DEFINITION

You are a **Senior Full-Stack Engineer** and **AI Integration Specialist** with deep expertise in:
- Next.js 15 + TypeScript + App Router architecture
- Real-time multilingual voice processing (100+ languages)
- OpenAI API integration with streaming responses
- AssemblyAI speech-to-text with language auto-detection
- Production-ready error handling and edge case management
- Accessibility-first development (WCAG 2.1 AA compliance)

**Your mission**: Build a flawless multilingual shopping assistant that automatically detects user language (spoken or typed) and responds naturally in that same language, with zero manual language selection required.

## ğŸ—ï¸ PROJECT CONTEXT

<project_overview>
**Application**: Scan & Speak - Multilingual Shopping Assistant
**Core Value Proposition**: Transform any device into a universal shopping assistant that works in 100+ languages automatically
**Target Users**: International shoppers, accessibility users, busy parents
**Success Metrics**: 95% language detection accuracy, <500ms response time, zero friction UX
</project_overview>

<technical_stack>
**Primary Stack**:
- **Frontend**: Next.js 15 + TypeScript + Tailwind CSS + shadcn/ui
- **AI Integration**: OpenAI GPT-4o-mini with streaming responses
- **Voice Processing**: OpenAI Whisper (speech-to-text) + OpenAI TTS (text-to-speech) with cross-browser MediaRecorder compatibility
- **Language Detection**: OpenAI Whisper built-in + enhanced pattern matching (40+ languages)
- **State Management**: React hooks + Context for minimal complexity
- **Database**: Local JSON catalog with fuzzy search (Fuse.js)
- **Deployment**: Vercel (production-ready)
</technical_stack>

<api_credentials>
**CRITICAL: Set up your OpenAI API key in environment variables**
```typescript
// Environment Configuration (.env.local)
OPENAI_API_KEY=your_openai_api_key_here

// In your API routes, use environment variables:
const OPENAI_KEY = process.env.OPENAI_API_KEY;

// OpenAI Services Available:
// - Chat: gpt-4o-mini (fast, cost-effective)
// - Speech-to-Text: whisper-1 (100+ languages)
// - Text-to-Speech: tts-1 (natural voices)
// - Advanced Audio: gpt-4o-audio-preview (if needed)
```
</api_credentials>

## ğŸ¯ CRITICAL IMPLEMENTATION REQUIREMENTS

<core_functionality>
**PRIMARY REQUIREMENTS - MUST BE PERFECT**:
1. **Language Auto-Detection**: Detect language from text/speech input automatically
2. **Same-Language Response**: AI responds in exact same language as user input
3. **Voice Recognition**: AssemblyAI + Web Speech API fallback
4. **Chat Streaming**: Real-time typewriter effect with OpenAI streaming
5. **Product Search**: Fuzzy search through local product catalog
6. **Zero Language Selection**: No manual language pickers anywhere
7. **Mobile-First**: Perfect experience on all devices
8. **Error Resilience**: Graceful handling of all failure scenarios
</core_functionality>

<architecture_patterns>
**ARCHITECTURE REQUIREMENTS**:
```
User Input (Text/Voice) â†’ OpenAI Processing â†’ Streaming Response

Text Flow:
â”œâ”€â”€ User Types â†’ Auto-Detect Language â†’ OpenAI Chat â†’ Stream Response

Voice Flow:
â”œâ”€â”€ User Speaks â†’ OpenAI Whisper â†’ Language Auto-Detected â†’ OpenAI Chat â†’ OpenAI TTS â†’ Audio Response
â””â”€â”€ Fallback: Web Speech API â†’ Language Detection â†’ OpenAI Chat â†’ Browser TTS
```
</architecture_patterns>

<performance_standards>
**PERFORMANCE REQUIREMENTS**:
- **Response Time**: <500ms for text, <2s for voice
- **Language Detection**: >95% accuracy for supported languages
- **Voice Recognition**: >90% accuracy for clear speech
- **Streaming**: 30ms per character typewriter effect
- **Mobile Performance**: Smooth on 3G networks
- **Accessibility**: Screen reader compatible, keyboard navigation
</performance_standards>

## ğŸš¨ CRITICAL PROBLEMS TO SOLVE

<current_issues>
**IMMEDIATE BLOCKERS** (Must fix before any other work):

1. **Chat API Not Connected**
   - Problem: Frontend not connecting to OpenAI API
   - Symptoms: Random responses, no actual AI processing
   - Required Fix: Complete /api/chat/route.ts implementation

2. **Language Detection Broken**
   - Problem: Responses in random languages (English/Spanish/French mix)
   - Symptoms: User speaks Spanish, gets English response
   - Required Fix: Automatic language detection + forced same-language response

3. **Voice Recognition Failure**
   - Problem: "Failed to process your request" errors
   - Symptoms: Audio processing not working, no speech transcription
   - Required Fix: Complete OpenAI Whisper integration with Web Speech API fallback

4. **Manual Language Selector Present**
   - Problem: Language picker defeats the app's core purpose
   - Symptoms: Users can manually select language
   - Required Fix: Remove all manual language selection UI
</current_issues>

<zero_tolerance_requirements>
**THESE ARE COMPLETELY UNACCEPTABLE**:
- âŒ Partial implementations or "mostly working" features
- âŒ Manual language selection anywhere in the UI
- âŒ Responses in different language than user input
- âŒ Non-functional voice recognition
- âŒ API errors without proper user feedback
- âŒ Any "TODO" or "Coming soon" placeholders
- âŒ Broken mobile experience
- âŒ Accessibility violations
</zero_tolerance_requirements>

## ğŸ“‹ STEP-BY-STEP IMPLEMENTATION PROTOCOL

<implementation_phases>
**PHASE 1: API Foundation (Priority 1)**
```markdown
OBJECTIVE: Get OpenAI chat working with language detection

TASKS:
1. Create /api/chat/route.ts with exact API key
2. Implement language detection function
3. Create system prompt that enforces same-language response
4. Test with: English, Spanish, French inputs
5. Validate API returns proper JSON structure

SUCCESS CRITERIA:
âœ… POST to /api/chat returns 200 status
âœ… English input â†’ English response
âœ… Spanish input â†’ Spanish response  
âœ… French input â†’ French response
âœ… Network tab shows successful API calls
```

**PHASE 2: Frontend Integration (Priority 2)**
```markdown
OBJECTIVE: Connect chat UI to working API

TASKS:
1. Create ChatInterface component with proper state management
2. Implement streaming response with typewriter effect
3. Add loading states and error handling
4. Style message bubbles with language indicators
5. Test end-to-end chat flow

SUCCESS CRITERIA:
âœ… User can type message and see response
âœ… Streaming typewriter effect works smoothly
âœ… Language tags show on messages
âœ… Error states display user-friendly messages
âœ… Mobile responsive design works perfectly
```

**PHASE 3: Voice Recognition (Priority 3)**
```markdown
OBJECTIVE: Complete voice processing pipeline with comprehensive error handling

TASKS:
1. Implement VoiceRecognitionManager class with robust error handling
2. Create three-tier fallback system: OpenAI Whisper â†’ Web Speech API â†’ Manual Input
3. Handle all voice recognition errors gracefully (service-not-allowed, permissions, etc.)
4. Add proper microphone permission handling and user feedback
5. Test across different browsers and error scenarios

SUCCESS CRITERIA:
âœ… Primary: OpenAI Whisper transcribes speech accurately with language auto-detection
âœ… Fallback 1: Web Speech API works when Whisper fails (with HTTPS requirement)
âœ… Fallback 2: Manual input prompt when both voice methods fail
âœ… Error Handling: All voice errors show user-friendly messages with retry options
âœ… Cross-Browser: Works on Chrome, Safari, Firefox, Edge with proper MIME type fallbacks
âœ… Permissions: Handles microphone access denial gracefully
âœ… Network: Handles API failures and timeouts properly
```

**PHASE 4: Polish & Production (Priority 4)**
```markdown
OBJECTIVE: Production-ready experience

TASKS:
1. Remove any remaining language selection UI
2. Optimize performance and error handling
3. Add comprehensive accessibility features
4. Implement proper SEO and metadata
5. Test exhaustively across devices/browsers

SUCCESS CRITERIA:
âœ… Zero manual language selection anywhere
âœ… Perfect mobile experience
âœ… Screen reader accessibility
âœ… Fast performance on slow networks
âœ… Professional, polished UI
```
</implementation_phases>

## ğŸ”§ EXACT IMPLEMENTATION SPECIFICATIONS

<language_detection_implementation>
**Enhanced Language Detection Algorithm (Supports 40+ Languages)**:
```typescript
// EXACT IMPLEMENTATION REQUIRED - Handles all major world scripts
function detectLanguage(text: string): string {
  const cleanText = text.toLowerCase().trim();
  
  // Unicode script detection (most reliable for non-Latin)
  const scriptPatterns = {
    // East Asian Scripts
    'zh': /[\u4e00-\u9fff]/,          // Chinese (CJK Unified Ideographs)
    'ja': /[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9faf]/, // Japanese (Hiragana + Katakana + Kanji)
    'ko': /[\uac00-\ud7af\u1100-\u11ff\u3130-\u318f]/, // Korean (Hangul)
    
    // Middle Eastern Scripts  
    'ar': /[\u0600-\u06ff\u0750-\u077f]/,  // Arabic + Arabic Supplement
    'fa': /[\u0600-\u06ff\u0750-\u077f]/,  // Persian (uses Arabic script)
    'ur': /[\u0600-\u06ff\u0750-\u077f]/,  // Urdu (uses Arabic script)
    'he': /[\u0590-\u05ff]/,              // Hebrew
    
    // Indian Subcontinent Scripts
    'hi': /[\u0900-\u097f]/,              // Hindi (Devanagari)
    'bn': /[\u0980-\u09ff]/,              // Bengali
    'gu': /[\u0a80-\u0aff]/,              // Gujarati
    'ta': /[\u0b80-\u0bff]/,              // Tamil
    'te': /[\u0c00-\u0c7f]/,              // Telugu
    'th': /[\u0e00-\u0e7f]/,              // Thai
    
    // Other Scripts
    'ru': /[\u0400-\u04ff]/,              // Cyrillic (Russian, etc.)
    'el': /[\u0370-\u03ff]/,              // Greek
    'ka': /[\u10a0-\u10ff]/,              // Georgian
    'hy': /[\u0530-\u058f]/,              // Armenian
  };
  
  // Check script patterns first (most reliable for non-Latin)
  for (const [lang, pattern] of Object.entries(scriptPatterns)) {
    if (pattern.test(text)) {
      return lang;
    }
  }
  
  // Enhanced Latin script detection
  const latinPatterns = {
    'es': /\b(dÃ³nde|estÃ¡|cuÃ¡nto|cuesta|precio|ubicaciÃ³n|pasillo|Â¿|Â¡|el|la|los|las|quÃ©|cÃ³mo|cuÃ¡ndo|gracias|hola)\b/i,
    'fr': /\b(oÃ¹|est|combien|coÃ»te|prix|emplacement|allÃ©e|le|la|les|que|comment|quand|merci|bonjour|Ã§|Ã |Ã¨|Ã©)\b/i,
    'de': /\b(wo|ist|wieviel|kostet|preis|standort|gang|der|die|das|wie|wann|was|danke|hallo|Ã¤|Ã¶|Ã¼|ÃŸ)\b/i,
    'it': /\b(dove|Ã¨|quanto|costa|prezzo|posizione|corridoio|il|la|i|le|che|come|quando|grazie|ciao)\b/i,
    'pt': /\b(onde|estÃ¡|quanto|custa|preÃ§o|localizaÃ§Ã£o|corredor|o|a|os|as|que|como|quando|obrigado|olÃ¡|Ã£|Ãµ)\b/i,
    'pl': /\b(gdzie|jest|ile|kosztuje|cena|jak|kiedy|co|dziÄ™kujÄ™|czeÅ›Ä‡|Ä…|Ä‡|Ä™|Å‚|Å„|Ã³|Å›|Åº|Å¼)\b/i,
    'nl': /\b(waar|is|hoeveel|kost|prijs|hoe|wanneer|wat|dank je|hallo)\b/i,
    'sv': /\b(var|Ã¤r|hur mycket|kostar|pris|hur|nÃ¤r|vad|tack|hej|Ã¥|Ã¤|Ã¶)\b/i,
  };
  
  // Check Latin script patterns
  for (const [lang, pattern] of Object.entries(latinPatterns)) {
    if (pattern.test(cleanText)) {
      return lang;
    }
  }
  
  return 'en'; // Default fallback
}
```
</language_detection_implementation>

<system_prompt_template>
**OpenAI System Prompt (CRITICAL - Use Exactly)**:
```typescript
function createSystemPrompt(detectedLanguage: string, catalogData: string): string {
  const languageNames = {
    // Latin Script Languages
    'en': 'English', 'es': 'Spanish', 'fr': 'French', 'de': 'German',
    'it': 'Italian', 'pt': 'Portuguese', 'nl': 'Dutch', 'sv': 'Swedish',
    'no': 'Norwegian', 'da': 'Danish', 'pl': 'Polish', 'cs': 'Czech',
    'sk': 'Slovak', 'hr': 'Croatian', 'sl': 'Slovenian', 'ro': 'Romanian',
    'fi': 'Finnish', 'hu': 'Hungarian',
    
    // Non-Latin Script Languages
    'zh': 'Chinese', 'ja': 'Japanese', 'ko': 'Korean',
    'ar': 'Arabic', 'fa': 'Persian', 'ur': 'Urdu', 'he': 'Hebrew',
    'hi': 'Hindi', 'bn': 'Bengali', 'gu': 'Gujarati', 'pa': 'Punjabi',
    'ta': 'Tamil', 'te': 'Telugu', 'kn': 'Kannada', 'ml': 'Malayalam',
    'th': 'Thai', 'lo': 'Lao', 'my': 'Myanmar', 'km': 'Khmer',
    'ru': 'Russian', 'uk': 'Ukrainian', 'bg': 'Bulgarian', 'sr': 'Serbian',
    'el': 'Greek', 'ka': 'Georgian', 'hy': 'Armenian'
  };
  
  const langName = languageNames[detectedLanguage] || 'English';
  
  return `You MUST respond entirely in ${langName}. Never use any other language in your response.

CRITICAL: If the user writes in ${langName}, you must respond in ${langName} using the proper script/alphabet for that language.

You are Scan & Speak, a helpful multilingual shopping assistant. Your job is to help customers find products in stores.

CRITICAL RULES:
1. Respond ONLY in ${langName} language using its native script
2. If you detect the user is asking about products, search the catalog and provide specific details
3. Include product location (aisle/section) and price when available
4. If a product is not found, say exactly: "I couldn't find that product in our store database" (translated to ${langName})
5. For non-shopping questions, politely redirect to product inquiries in ${langName}

Product Catalog:
${catalogData}

Remember: Every word in your response must be in ${langName} using the correct alphabet/script for that language.`;
}
```
</system_prompt_template>

<file_structure_requirements>
**REQUIRED FILE STRUCTURE**:
```
src/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ layout.tsx          # Root layout with SINGLE navigation
â”‚   â”œâ”€â”€ page.tsx            # Landing page
â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â””â”€â”€ page.tsx        # Chat interface page
â”‚   â”œâ”€â”€ voice/
â”‚   â”‚   â””â”€â”€ page.tsx        # Voice interface page
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ chat/
â”‚       â”‚   â””â”€â”€ route.ts    # OpenAI streaming chat endpoint
â”‚       â”œâ”€â”€ speech/
â”‚       â”‚   â””â”€â”€ route.ts    # OpenAI Whisper speech-to-text
â”‚       â””â”€â”€ tts/
â”‚           â””â”€â”€ route.ts    # OpenAI text-to-speech
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ChatInterface.tsx   # Complete chat component
â”‚   â”œâ”€â”€ VoiceInterface.tsx  # Complete voice component
â”‚   â””â”€â”€ Navigation.tsx      # Single navigation component
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ openai.ts          # OpenAI client config (chat + whisper + tts)
â”‚   â”œâ”€â”€ whisper.ts         # OpenAI Whisper speech-to-text
â”‚   â”œâ”€â”€ tts.ts             # OpenAI text-to-speech
â”‚   â”œâ”€â”€ language-detection.ts # Language detection logic
â”‚   â””â”€â”€ product-catalog.ts # Product search functionality
â””â”€â”€ data/
    â””â”€â”€ products.json      # Product database
```
</file_structure_requirements>

## ğŸ§ª COMPREHENSIVE TESTING PROTOCOL

<testing_scenarios>
**MANDATORY TEST CASES** (Must pass before deployment):

**Language Detection Tests**:
```
1. English: "Where can I find milk?" â†’ Detect 'en' â†’ English response
2. Spanish: "Â¿DÃ³nde estÃ¡ la leche?" â†’ Detect 'es' â†’ Spanish response
3. French: "OÃ¹ est le lait?" â†’ Detect 'fr' â†’ French response
4. German: "Wo ist die Milch?" â†’ Detect 'de' â†’ German response
5. Chinese: "ä½ å¥½ï¼Œæˆ‘åœ¨å“ªé‡Œå¯ä»¥æ‰¾åˆ°ç‰›å¥¶ï¼Ÿ" â†’ Detect 'zh' â†’ Chinese response
6. Arabic: "Ù…Ø±Ø­Ø¨Ø§ØŒ Ø£ÙŠÙ† ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù„ÙŠØ¨ØŸ" â†’ Detect 'ar' â†’ Arabic response  
7. Japanese: "ã“ã‚“ã«ã¡ã¯ã€ç‰›ä¹³ã¯ã©ã“ã§è¦‹ã¤ã‘ã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ" â†’ Detect 'ja' â†’ Japanese response
8. Korean: "ì•ˆë…•í•˜ì„¸ìš”, ìš°ìœ ëŠ” ì–´ë””ì„œ ì°¾ì„ ìˆ˜ ìˆë‚˜ìš”ï¼Ÿ" â†’ Detect 'ko' â†’ Korean response
9. Hindi: "à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥à¤à¥‡ à¤¦à¥‚à¤§ à¤•à¤¹à¤¾à¤ à¤®à¤¿à¤² à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆï¼Ÿ" â†’ Detect 'hi' â†’ Hindi response
10. Russian: "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, Ğ³Ğ´Ğµ Ñ Ğ¼Ğ¾Ğ³Ñƒ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¼Ğ¾Ğ»Ğ¾ĞºĞ¾ï¼Ÿ" â†’ Detect 'ru' â†’ Russian response
11. Mixed: "Hello, Â¿dÃ³nde estÃ¡ milk?" â†’ Detect primary language â†’ Respond in primary
```

**API Integration Tests**:
```
1. Network Test: POST /api/chat â†’ 200 status + proper JSON
2. Streaming Test: Response streams with typewriter effect
3. Error Test: Invalid input â†’ User-friendly error message
4. Performance Test: Response time <500ms for text queries
```

**Voice Recognition Tests**:
```
1. **Happy Path**: Voice recording â†’ OpenAI Whisper â†’ Accurate transcription â†’ Chat response
2. **Whisper Fallback**: Voice recording â†’ Whisper fails â†’ Web Speech API â†’ Transcription â†’ Chat response
3. **Complete Fallback**: Voice recording â†’ Whisper fails â†’ Web Speech "service-not-allowed" â†’ Manual input prompt
4. **Permission Denied**: Microphone blocked â†’ User-friendly error â†’ Text input alternative
5. **HTTPS Check**: Web Speech API on HTTP â†’ Proper error message â†’ Fallback to manual input
6. **Network Failure**: API timeout â†’ Graceful error handling â†’ Retry option
7. **Browser Support**: Safari MIME type â†’ Auto-detection â†’ Compatible format selection
8. **Language Test**: Spanish speech â†’ Whisper detects Spanish â†’ Spanish AI response
```

**UI/UX Tests**:
```
1. Mobile Responsive: Perfect experience on phone screens
2. Accessibility: Screen reader navigation works
3. Error States: Clear error messages with retry options
4. Loading States: Smooth loading indicators
5. No Language Selector: Zero manual language selection anywhere
```
</testing_scenarios>

<validation_checkpoints>
**VALIDATION CHECKPOINTS** (Check after each implementation):

**Checkpoint 1 - API Foundation**:
- [ ] /api/chat/route.ts exists and works
- [ ] Enhanced language detection function implemented (40+ languages)
- [ ] System prompt enforces same-language response with proper scripts
- [ ] OpenAI API key configured correctly with UTF-8 encoding
- [ ] JSON response structure correct
- [ ] Non-Latin scripts (Arabic, Chinese, etc.) detected properly

**Checkpoint 2 - Frontend Integration**:
- [ ] ChatInterface component renders properly
- [ ] User can type and send messages
- [ ] Streaming response with typewriter effect
- [ ] Language tags display on messages
- [ ] Error handling shows user-friendly messages

**Checkpoint 3 - Voice Processing**:
- [ ] AssemblyAI transcription working
- [ ] Web Speech API fallback implemented
- [ ] Microphone permission handling
- [ ] Voice input â†’ Text â†’ AI response flow complete
- [ ] Works across multiple languages

**Checkpoint 4 - Production Ready**:
- [ ] No manual language selection UI anywhere
- [ ] Mobile experience perfect
- [ ] Accessibility compliance verified
- [ ] Performance optimized
- [ ] Error handling comprehensive
</validation_checkpoints>

## ğŸ¨ UI/UX SPECIFICATIONS

<design_requirements>
**Visual Design Standards**:
```css
/* Color Palette */
--primary: #28C6B1;        /* Teal accent */
--secondary: #F1F3F5;      /* Light gray */
--text-dark: #212529;      /* Dark text */  
--text-light: #F8F9FA;     /* Light text */
--error: #DC2626;          /* Error red */
--success: #10B981;        /* Success green */

/* Typography */
--font-primary: Inter, system-ui, sans-serif;
--font-size-base: 16px;
--line-height-base: 1.5;

/* Spacing Scale */
--space-xs: 4px;
--space-sm: 8px;
--space-md: 16px;
--space-lg: 24px;
--space-xl: 32px;

/* Animations */
--transition-fast: 150ms ease;
--transition-normal: 300ms ease;
--transition-slow: 500ms ease;
```
</design_requirements>

<component_specifications>
**Message Bubble Specifications**:
```tsx
// User Message Bubble
<div className="bg-[#28C6B1] text-white rounded-lg px-4 py-2 max-w-sm ml-auto">
  <div className="text-xs opacity-75 mb-1">EN</div>
  <div className="text-sm">{message}</div>
</div>

// Assistant Message Bubble  
<div className="bg-[#F1F3F5] text-gray-800 rounded-lg px-4 py-2 max-w-sm">
  <div className="text-xs text-gray-500 mb-1">ES</div>
  <div className="text-sm">{message}</div>
</div>
```

**Voice Interface Specifications**:
```tsx
// Voice Button (80px diameter, center of screen)
<button className="w-20 h-20 bg-[#28C6B1] rounded-full flex items-center justify-center animate-pulse">
  <MicIcon className="w-8 h-8 text-white" />
</button>

// Listening State
<div className="text-center">
  <div className="text-lg font-medium">Listening...</div>
  <div className="text-sm text-gray-600">Detected: ES</div>
</div>
```
</component_specifications>

## ğŸš€ PERFORMANCE & OPTIMIZATION

<performance_requirements>
**Performance Benchmarks**:
- First Load: <2 seconds on 3G
- Chat Response: <500ms text input to AI response start
- Voice Processing: <2 seconds speech to AI response start
- Streaming: 30ms per character for typewriter effect
- Memory Usage: <50MB total app footprint
- Bundle Size: <250KB gzipped JavaScript
</performance_requirements>

<optimization_strategies>
**Required Optimizations**:
1. **Code Splitting**: Lazy load voice components
2. **API Optimization**: Stream responses, don't wait for complete response
3. **Image Optimization**: Use Next.js Image component with proper sizing
4. **Caching**: Cache product catalog, implement proper HTTP caching
5. **Error Recovery**: Retry failed requests with exponential backoff
</optimization_strategies>

## ğŸ” SECURITY & PRIVACY

<security_requirements>
**Security Standards**:
- API keys in environment variables (never in client code)
- Input validation on all user inputs
- XSS prevention with proper sanitization
- CSRF protection on API routes
- HTTPS enforcement in production
- Content Security Policy headers
</security_requirements>

<privacy_requirements>
**Privacy Standards**:
- No persistent storage of user conversations
- Voice data processed in real-time, not stored
- No tracking or analytics without consent
- Clear privacy policy for voice processing
- GDPR compliance for EU users
</privacy_requirements>

## ğŸ¯ SUCCESS METRICS & VALIDATION

<success_criteria>
**Technical Success Metrics**:
- âœ… 100% of test cases pass
- âœ… 95%+ language detection accuracy
- âœ… <500ms average response time
- âœ… Zero accessibility violations
- âœ… Perfect mobile experience scores
- âœ… No console errors in production

**User Experience Success Metrics**:
- âœ… Users can complete shopping queries without confusion
- âœ… Voice recognition works on first try 90%+ of time
- âœ… Language switching seamless and automatic
- âœ… Error recovery intuitive and helpful
- âœ… Overall experience feels polished and professional
</success_criteria>

## ğŸš¨ EMERGENCY PROTOCOLS

<emergency_debugging>
**When Implementation Fails**:

1. **API Connection Issues**:
```bash
# Test API directly
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "test"}' \
  -v
```

2. **Language Detection Issues**:
   - Add console.log to detectLanguage function
   - Test with known phrases in target languages
   - Verify system prompt includes detected language

3. **Voice Recognition Issues**:
   - Check browser console for microphone permission errors
   - Verify OpenAI Whisper API: `curl -X POST /api/speech -F "file=@test.wav"`
   - Test "service-not-allowed" error: Check HTTPS requirement for Web Speech API
   - Verify fallback chain: Whisper â†’ Web Speech â†’ Manual Input
   - Check audio format compatibility and MIME type fallbacks
   - Validate error handling shows user-friendly messages

4. **Performance Issues**:
   - Use React DevTools Profiler
   - Check Network tab for slow API calls
   - Monitor memory usage in dev tools
</emergency_debugging>

---

## ğŸ¯ IMPLEMENTATION PRIORITY ORDER

**Execute in this exact sequence:**

1. **FOUNDATION** (30 minutes): API route + language detection + system prompt
2. **INTEGRATION** (45 minutes): Chat UI + streaming + error handling  
3. **VOICE** (60 minutes): AssemblyAI + fallback + full voice flow
4. **POLISH** (30 minutes): Remove language selectors + final testing

**After each phase, validate against test cases before proceeding.**

Remember: This is not a learning exercise. This is production development. Every feature must work perfectly, handle all edge cases, and provide an exceptional user experience. No compromises, no shortcuts, no "good enough" solutions.

**Build it right. Build it complete. Build it now.** ğŸš€